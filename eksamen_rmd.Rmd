---
title: "Research on NFL data"
author: "Team 2 (Patrick Berthelsen, Natascha Svarre, Alex Rye & Nino Krupic)"
date: "21/11/2021"
output: html_document
---
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## 3 Data
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
pacman::p_load(
  tidyverse, #tidyverse includes ggplot2, dplyr, forcats and more. 
  caret, 
  ggvis,
  rpart.plot,
  ggmap, #for plotting maps
  RColorBrewer, #provide pretty colour palettes
  lubridate, #for converting time variables into date format
  hrbrthemes, #for more style themes
  data.table, #for working with data.table
  tidytext, #for text mining functions such as unnest()
  wordcloud,
  tidyquant, #for theme_tq
  prismatic, #for color handling in plots
  knitr,
  devtools,
  plotrix,
  dynlm,
  olsrr,
  car
)
```



### 3.1 Data cleaning

#### 3.1.1 Data on NFL season standings 2000-2019
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
standings <- read.csv("standings.csv")
standings <- as.data.frame(standings)

sb_winners <- standings[ which(standings$sb_winner=="Won Superbowl"), ] #Subset of data including only SB-winners

standings[ ,c('year', 'team')] <- list(NULL) #deleting irrelavent variables

standings$playoffs <- ifelse(standings$playoffs == "Playoffs", 1 ,0)
standings$sb_winner <- ifelse(standings$sb_winner == "Won Superbowl", 1 ,0)

str(standings)
```

#### 3.1.2 Data on NFL game results 2000-2019
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
games <- read.csv("games.csv")
games <- as.data.frame(games)
games$winners <- stringr::word(games$winner, -1)
games[ ,c('winner', 'home_team', 'away_team', 'day', 'date', 'time', 'home_team_city', 'away_team_city', 'year', 'week')] <- list(NULL) #deleting irrelavent variables
games$tie <- ifelse(games$tie == "NA", 0 ,1)
games$tie[is.na(games$tie)] <- 0
games$winners <- ifelse(games$tie == 1, "No Winner", games$winners)
games$home_win <- ifelse(games$winners == games$home_team_name ,1,0)

games$pts_home <- ifelse(games$home_win == 1, games$pts_win, games$pts_loss) #creating variable: home team points
games$pts_away <- ifelse(games$home_win == 0, games$pts_win, games$pts_loss) #creating variable: away team points 

games$yds_home <- ifelse(games$home_win == 1, games$yds_win, games$yds_loss) #creating variable: home team yards
games$yds_away <- ifelse(games$home_win == 0, games$yds_win, games$yds_loss) #creating variable: away team yards

games$turnovers_home <- ifelse(games$home_win == 1, games$turnovers_win, games$turnovers_loss) #creating variable: home team turnovers
games$turnovers_away <- ifelse(games$home_win == 0, games$turnovers_win, games$turnovers_loss) #creating variable: away team turnovers

games[ ,c('pts_win', 'pts_loss', 'yds_win', 'yds_loss', 'turnovers_win', 'turnovers_loss')] <- list(NULL) #deleting old variables

str(games)


```




## 5 Analysis
### 5.1 EDA 
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}

#Barplot of wins in regular season by the SB-winner 2000-2019
sb_winners %>% 
  ggplot(aes(x = year, y = wins)) + 
  geom_line() +
  labs(title = "Regular season wins by SB-winners 2000-2019", x = "Year", y = "Games won") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5))

#Barplot of SB-wins per team 
ggplot(data = sb_winners) + 
  geom_bar(aes(team_name)) + 
  theme_linedraw() + 
  labs(title = "Super Bowls won per team 2000-2019", x = "Teams", y = "Super Bowl wins") +
  theme(plot.title = element_text(hjust = 0.5))

#Barplot of total wins per team
ggplot(data = games) + 
  geom_bar(aes(x = fct_infreq(winners))) + 
  theme_linedraw() + 
  labs(title = "Games won per team 2000-2019", x = "Teams", y = "Games won") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  coord_flip() 



#Creating dataset of only winners of games
game_winners <- games["winners"] %>% 
  group_by(winners) %>% 
  mutate(total_wins = n()) %>% 
  ungroup() %>% 
  count(winners, sort = TRUE)

#Wordcloud
wordcloud(words = game_winners$winners, freq = game_winners$n, min.freq = 0,
          max.words=200, random.order=FALSE, rot.per=0.3, 
          colors=brewer.pal(9, "GnBu")[3:9], scale=c(3,.5))

#Correlation between wins and SB wins
corr_subset <- game_winners[1:32,] 
corr_subset$sb_wins <- c(6,2,1,1,2,1,1,1,1,1,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0)

corr_subset %>% 
    ggplot(aes(x = n, y = sb_wins)) + 
    geom_point() +
    labs(title = "Correlation between total wins and Super Bowl wins", x = "Games won", y = "Super Bowls won") + 
    theme(plot.title = element_text(hjust = 0.5)) 




#Statistics on games dataset
table(games$home_win)
3045/(3045+2279)

stats <- data.frame(HomeTeam=round(c(mean(games$pts_home),mean(games$yds_home),mean(games$turnovers_home)), digits = 2), AwayTeam=round(c(mean(games$pts_away),mean(games$yds_away),mean(games$turnovers_away)), digits = 2))
row.names(stats) <- c("Points", "Yards", "Turnovers")
stats


```
 
 








## 6 Prediction models
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}

RNGkind(sample.kind = "Rounding")
set.seed(356)

#Training and test dataset for games with home_win as success

games$home_win <- as.factor(games$home_win)

# Training and test dataset  
games_index <- createDataPartition(games$home_win, p=0.8, list=FALSE) #Index

games_train <- games[games_index,]
games_train[ ,c('away_team_name', 'home_team_name', 'winners')] <- list(NULL)

games_test <- games[-games_index,]
games_test[ ,c('away_team_name', 'home_team_name', 'winners')] <- list(NULL)


```

### 6.1 Games dataset

#### 6.1.1 Logistic regression
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}

reg <- glm(home_win ~ pts_home+pts_away+yds_home+yds_away+turnovers_home+turnovers_away, data = games_train, family = "binomial")

summary(reg)

vif(reg)

#since there is sign of heavy multicorrelation in both pts_home and pts_away(>200), we remove the pts_away variable, since we want to focus on home team performance.

games_train[ ,c('away_team_name', 'home_team_name', 'winners', 'pts_away')] <- list(NULL)
games_test[ ,c('away_team_name', 'home_team_name', 'winners', 'pts_away')] <- list(NULL)

reg <- glm(home_win ~ pts_home+yds_home+yds_away+turnovers_home+turnovers_away, data = games_train, family = "binomial")

#vif(reg) - marked as a comment since the code wont run until some perfect correleated variables are removed.

summary(reg)

importance_reg <- varImp(reg)

importance_reg %>%
  arrange(desc(Overall))

probs_reg <- predict(reg, newdata = games_test, type = "response")
pred_reg <- ifelse(probs_reg > 0.5, 1, 0)

confusionMatrix(factor(pred_reg), factor(games_test$home_win), positive = as.character(1))

```


#### 6.1.2 SVM (Support Vector Machine)
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}

model_svm<- train(games_train[, 3:7], games_train[, 2], method = 'svmLinear')
predictions_svm <- predict(object = model_svm, games_test[,3:7])

# Confusion matrix 
confusionMatrix(predictions_svm, games_test[,2], positive = "1")

varimp_svm <- varImp(model_svm)
plot(varimp_svm, main="Variable Importance with support vector machine classifier")


```


#### 6.1.3 K-nearest neighbour
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
# Train a model using KNN algorithm
model_knn_norm <- train(games_train[, 3:7], games_train[, 2], method = 'knn', preProcess = c("range")) 
 
# Predict the labels of the test set
predictions_knn_norm <- predict(object = model_knn_norm, games_test[,3:7]) #the first 11 variables

# Confusion matrix 
confusionMatrix(predictions_knn_norm, games_test[,2], positive = "1") #the 12th variable is our outcome/predictive variable

varimp_knn_norm <- varImp(model_knn_norm)
plot(varimp_knn_norm, main="Variable Importance with k-neares neighbor classifier")

```





### 6.2 Standings dataset (playoffs as success parameter)
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}

standings$playoffs <- as.factor(standings$playoffs)
standings$sb_winner <- as.factor(standings$sb_winner)


# Training and test dataset for standings with playoffs as success parameter

standings_index <- createDataPartition(standings$playoffs, p=0.8, list=FALSE) #Index

standings_train <- standings[standings_index,]
standings_test <- standings[-standings_index,]
```


#### 6.2.1 Logistic regression
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}


reg2 <- glm(playoffs ~ wins+loss+points_for+points_against+points_differential+margin_of_victory+strength_of_schedule+simple_rating+offensive_ranking+defensive_ranking, data = standings_train, family = "binomial")

summary(reg2)

#vif(reg2) - marked as a comment since the code wont run until some perfect correleated variables are removed.


#Firstly, the win and loss variables are removed because it is those variable that determinate whether you proceed to the playoffs or not. therefore, in order to find factors that has doesn't have the direct linkage between proceeding or not proceeding to playoffs, win and loss are removed. Also,since points_differential, margin_of_victory, simple_rating, offensive_ranking and defensive_ranking all have roots in points_for and points_against, we have to remove them to avoid multicollinearity.


standings_train[, c('wins', 'loss', 'points_differential', 'margin_of_victory', 'simple_rating', 'offensive_ranking', 'defensive_ranking')] <- list(NULL)
standings_test[, c('wins', 'loss', 'points_differential', 'margin_of_victory', 'simple_rating', 'offensive_ranking', 'defensive_ranking')] <- list(NULL)

reg2 <- glm(playoffs ~ points_for+points_against+strength_of_schedule, data = standings_train, family = "binomial")

vif(reg2)

summary(reg2)

importance_reg2 <- varImp(reg2)


importance_reg2 %>%
  arrange(desc(Overall))

probs_reg2 <- predict(reg2, newdata = standings_test, type = "response")
pred_reg2 <- ifelse(probs_reg2 > 0.5, 1, 0)

confusionMatrix(factor(pred_reg2), factor(standings_test$playoffs), positive = as.character(1))



```



#### 6.2.2 SVM
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
model_svm2 <- train(standings_train[, 2:4], standings_train[, 5], method = 'svmLinear')
predictions_svm2 <- predict(object = model_svm2, standings_test[,2:4])

# Confusion matrix 
confusionMatrix(predictions_svm2, standings_test[,5], positive = "1")

varimp_svm2 <- varImp(model_svm2)
plot(varimp_svm2, main="Variable Importance with support vector machine classifier")
```




#### 6.2.3 KNN
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
# Train a model using KNN algorithm
model_knn_norm2 <- train(standings_train[, 2:4], standings_train[, 5], method = 'knn',preProcess = c("scale")) 
 
# Predict the labels of the test set
predictions_knn_norm2 <- predict(object = model_knn_norm2, standings_test[,2:4]) #the first 11 variables

# Confusion matrix 
confusionMatrix(predictions_knn_norm2, standings_test[,5], positive = "1") #the 12th variable is our outcome/predictive variable

varimp_knn_norm2 <- varImp(model_knn_norm2)
plot(varimp_knn_norm2, main="Variable Importance with k-neares neighbor classifier")
```





















### 6.3 Standings dataset (winning superbowl as success parameter)
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
# Training and test dataset for standings with playoffs as success parameter
standings_index2 <- createDataPartition(standings$sb_winner, p=0.8, list=FALSE) #Index

standings_train2 <- standings[standings_index2,]
standings_test2 <- standings[-standings_index2,]
```

#### 6.3.1 Logistic Regression
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}

reg3 <- glm(sb_winner ~ wins+loss+points_for+points_against+points_differential+margin_of_victory+strength_of_schedule+simple_rating+offensive_ranking+defensive_ranking, data = standings_train2, family = "binomial")

summary(reg3)

#vif(reg3) - marked as a comment since the code wont run until some perfect correleated variables are removed.

#Using the vif(variance inflation factor) to test for multicollinearity, the variables loss, points_differential, margin_of_victory, simple_rating, offensive_ranking and defensive_ranking end up being removed from the regression model. also, most of the variables are in direct linkage with points_for or points_against.

standings_train2[, c('loss', 'points_differential', 'margin_of_victory', 'simple_rating', 'offensive_ranking', 'defensive_ranking')] <- list(NULL)
standings_test2[, c('loss', 'points_differential', 'margin_of_victory', 'simple_rating', 'offensive_ranking', 'defensive_ranking')] <- list(NULL)

reg3 <- glm(sb_winner ~ wins+points_for+points_against+strength_of_schedule, data = standings_train2, family = "binomial")

vif(reg3)

summary(reg3)

importance_reg3 <- varImp(reg3)

importance_reg3 %>%
  arrange(desc(Overall))

probs_reg3 <- predict(reg3, newdata = standings_test2, type = "response")
pred_reg3 <- ifelse(probs_reg3 > 0.5, 1, 0)

confusionMatrix(factor(pred_reg2), factor(standings_test$playoffs), positive = as.character(1))


#hypothesis 5 - Since point against is significant we can conclude that having a good defence is the most important, since points against you have a larger negative impact on your chances of winning super bowl compared to the positive impact that points that you score have. 

```


#### 6.3.2 SVM
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
model_svm3 <- train(standings_train2[, 2:5], standings_train2[, 7], method = 'svmLinear') 
predictions_svm3 <- predict(object = model_svm3, standings_test2[,2:5])

# Confusion matrix 
confusionMatrix(predictions_svm3, standings_test2[,7], positive = "1")

varimp_svm3 <- varImp(model_svm3)
plot(varimp_svm3, main="Variable Importance with support vector machine classifier")
```



#### 6.3.3 KNN
```{r echo=FALSE, results='hide',message=FALSE, warning=FALSE}
# Train a model using KNN algorithm
model_knn_norm3 <- train(standings_train2[, 2:5], standings_train2[, 7], method = 'knn')
 
# Predict the labels of the test set
predictions_knn_norm3 <- predict(object = model_knn_norm3, standings_test2[,2:5]) 

# Confusion matrix 
confusionMatrix(predictions_knn_norm3, standings_test2[,7], positive = "1") 

varimp_knn_norm3 <- varImp(model_knn_norm3)
plot(varimp_knn_norm3, main="Variable Importance with k-neares neighbor classifier")
```







## 7 Results
