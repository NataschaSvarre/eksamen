---
title: "Research on NFL data"
author: "Team 2 (Patrick Berthelsen, Natascha Svarre, Alex Rye & Nino Krupic)"
date: "21/11/2021"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1 Introduction
## 2 Theory
## 3 Data 

```{r}

pacman::p_load(
  tidyverse, #tidyverse includes ggplot2, dplyr, forcats and more. 
  caret, 
  ggvis,
  rpart.plot,
  ggmap, #for plotting maps
  RColorBrewer, #provide pretty colour palettes
  lubridate, #for converting time variables into date format
  hrbrthemes, #for more style themes
  data.table, #for working with data.table
  tidytext, #for text mining functions such as unnest()
  wordcloud,
  tm, #for removing stopwords
  ggdist, #for raincloud plots
  tidyquant, #for theme_tq
  prismatic, #for color handling in plots
  gganimate, #for animated plots
  gifski, #for rendering animated plots as GIFs instead of single frames
  knitr,
  naivebayes,
  randomForest,
  tree,
  devtools,
  plotrix,
  reprtree
)


```


### 3.1 Data cleaning
```{r}

standings <- read.csv("standings.csv")
standings$team <- NULL

head(standings)
str(standings) # Dispalys the structure of our data.




games <- read.csv("games.csv")
games$winners <- stringr::word(games$winner, -1)
games[ ,c('winner', 'home_team', 'away_team', 'day', 'date', 'time', 'home_team_city', 'away_team_city', 'year', 'week')] <- list(NULL) #deleting irrelavent variables
games$tie <- ifelse(games$tie == "NA", 0 ,1)
games$tie[is.na(games$tie)] <- 0
games$winners <- ifelse(games$tie == 1, "No Winner", games$winners)
games$home_win <- ifelse(games$winners == games$home_team_name ,1,0)
games$home_win <- as.factor(games$home_win)

games$pts_home <- ifelse(games$home_win == 1, games$pts_win, games$pts_loss) #creating variable: home team points
games$pts_away <- ifelse(games$home_win == 0, games$pts_win, games$pts_loss) #creating variable: away team points 
games$pts_home_margin <- games$pts_home - games$pts_away                     #creating variable: home team points margin
 
games$yds_home <- ifelse(games$home_win == 1, games$yds_win, games$yds_loss) #creating variable: home team yards
games$yds_away <- ifelse(games$home_win == 0, games$yds_win, games$yds_loss) #creating variable: away team yards
games$yds_home_margin <- games$yds_home - games$yds_away                     #creating variable: home team yards margin

games$turnovers_home <- ifelse(games$home_win == 1, games$turnovers_win, games$turnovers_loss) #creating variable: home team turnovers
games$turnovers_away <- ifelse(games$home_win == 0, games$turnovers_win, games$turnovers_loss) #creating variable: away team turnovers
games$turnovers_home_margin <- games$turnovers_home - games$turnovers_away                     #creating variable: home team turnovers margin

games[ ,c('pts_win', 'pts_loss', 'yds_win', 'yds_loss', 'turnovers_win', 'turnovers_loss')] <- list(NULL) #deleting old variables

str(games)


# Training and test dataset  
games_index <- createDataPartition(games$home_win, p=0.8, list=FALSE) #Index

games_train <- games[games_index,]
games_train[ ,c('away_team_name', 'home_team_name', 'winners')] <- list(NULL)

games_test <- games[-games_index,]
games_test[ ,c('away_team_name', 'home_team_name', 'winners')] <- list(NULL)

```


## 4 Hypotheses

## 5 Analysis
### 5.1 EDA 
```{r}
#Subset of SB-Winners
sb_winners <- standings[ which(standings$sb_winner=="Won Superbowl"), ]


#Barplot of wins in regular season by the SB-winner 2000-2019
sb_winners %>% 
  ggplot(aes(x = year, y = wins)) + 
  geom_line() +
  labs(title = "Regular season wins by SB-winners 2000-2019", x = "Year", y = "Games won") +
  theme_linedraw() + 
  theme(plot.title = element_text(hjust = 0.5))

#Barplot of SB-wins per team
ggplot(data = sb_winners) + 
  geom_bar(aes(team_name)) + 
  theme_linedraw() + 
  labs(title = "Number of Superbowl won pr team", x = "Teams", y = "SB won") +
  theme(plot.title = element_text(hjust = 0.5))

#Barplot of total wins per team
ggplot(data = games) + 
  geom_bar(aes(x = fct_infreq(winners))) + 
  theme_linedraw() + 
  labs(title = "Number of games won pr team", x = "Teams", y = "Games won") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  coord_flip() 



  #Correlation between points and yards
games %>% 
    ggplot(aes(x = yds_home, y = pts_home)) + 
    geom_point()

games %>% 
    ggplot(aes(x = yds_away, y = pts_away)) + 
    geom_point()


```




#### 5.1.1 Wordcloud
```{r}
#Dataset of only winners of games

library(RColorBrewer)         
library(wordcloud)  

game_winners <- games["winners"] %>% 
  group_by(winners) %>% #grouping each word so that we can count number of times the word appears below
  mutate(total_wins = n()) %>% #counting number of times the word appears
  ungroup() %>% #ungrouping the words again
  count(winners, sort = TRUE)

wordcloud(words = game_winners$winners, freq = game_winners$n, min.freq = 0,
          max.words=200, random.order=FALSE, rot.per=0.3, 
          colors=brewer.pal(9, "GnBu")[3:9], scale=c(3,.5))
```




### 5.2 Prediction models
```{r}
set.seed(356)
```



#### 5.2.1 Naive Bayes
```{r}
model_nb <- train(games_train[, 3:11], games_train[, 2], method = 'naive_bayes')  #preProcess=c("center", "scale") 
predictions_nb <- predict(object = model_nb, games_test[,3:11])

# Confusion matrix 
confusionMatrix(predictions_nb, games_test[,2], positive = "1")

varimp_nb <- varImp(model_nb)
plot(varimp_nb, main="Variable Importance with naive bayes classifier")
```



#### 5.2.2 SVM (Support Vector Machine)
```{r}


model_svm<- train(games_train[, 3:11], games_train[, 2], method = 'svmLinear',preProcess=c("BoxCox")) #preProcess=c("center", "scale")
predictions_svm <- predict(object = model_svm, games_test[,3:11])

# Confusion matrix 
confusionMatrix(predictions_svm, games_test[,2], positive = "1")

varimp_svm <- varImp(model_svm)
plot(varimp_svm, main="Variable Importance with support vector machine classifier")



```

#### 5.2.3 K-nearest neighbour
```{r}
# Train a model using KNN algorithm
model_knn_norm <- train(games_train[, 3:11], games_train[, 2], method = 'knn',preProcess = c("range")) 
 
# Predict the labels of the test set
predictions_knn_norm <- predict(object = model_knn_norm, games_test[,3:11]) #the first 11 variables

# Confusion matrix 
confusionMatrix(predictions_knn_norm, games_test[,2], positive = "1") #the 12th variable is our outcome/predictive variable

varimp_knn_norm <- varImp(model_knn_norm)
plot(varimp_knn_norm, main="Variable Importance with k-neares neighbor classifier")

```

#### 5.2.4 Decision tree
```{r}

games_train[ ,c('pts_home', 'pts_away', 'pts_home_margin')] <- list(NULL) #deleting variables
games_test[ ,c('pts_home', 'pts_away', 'pts_home_margin')] <- list(NULL) #deleting variables



#Decision tree
model_dt <- train(games_train[, 3:8], games_train[, 2], method = 'rf') #preProcess=c("center", "scale")
plot(model_dt)

predictions_dt <- predict(object = model_dt, games_test[,3:8])

# Confusion matrix 
confusionMatrix(predictions_dt, games_test[,2], positive = "1")

#Variable importance plot
varimp_dt <- varImp(model_dt)
plot(varimp_dt, main="Variable Importance with decision tree classifier")

#Let's take a look at how the decision tree that the model has used to predict ended up looking!
pr(model_dt$finalModel, box.palette = "Reds", tweak = 1.2) #the "tweak" parameter controls text size, and "box.palette" controls node colour


tree_func <- function(final_model,  
                      tree_num) {
  
  # get tree by index
  tree <- randomForest::getTree(final_model, 
                                k = tree_num, 
                                labelVar = TRUE) %>%
    tibble::rownames_to_column() %>%
    # make leaf split points to NA, so the 0s won't get plotted
    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))
  
  # prepare data frame for graph
  graph_frame <- data.frame(from = rep(tree$rowname, 2),
                            to = c(tree$`left daughter`, tree$`right daughter`))
  
  # convert to graph and delete the last node that we don't want to plot
  graph <- graph_from_data_frame(graph_frame) %>%
    delete_vertices("0")
  
  # set node labels
  V(graph)$node_label <- gsub("_", " ", as.character(tree$`split var`))
  V(graph)$leaf_label <- as.character(tree$prediction)
  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))
  
  # plot
  plot <- ggraph(graph, 'dendrogram') + 
    theme_bw() +
    geom_edge_link() +
    geom_node_point() +
    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +
    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = "white") +
    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, 
					repel = TRUE, colour = "white", fontface = "bold", show.legend = FALSE) +
    theme(panel.grid.minor = element_blank(),
          panel.grid.major = element_blank(),
          panel.background = element_blank(),
          plot.background = element_rect(fill = "white"),
          panel.border = element_blank(),
          axis.line = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          plot.title = element_text(size = 18))
  
  print(plot)
}



tree_num <- which(model_dt$finalModel$forest$ndbigtree == min(model_dt$finalModel$forest$ndbigtree))

tree_func(final_model = model_dt$finalModel, tree_num)
```

```{r}

model_rf <- randomForest(margin~., data = model.matrix(games))

plot(model_rf)
```

